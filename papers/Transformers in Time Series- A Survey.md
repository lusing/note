# Transformers in Time Series: A Survey

## Abstract

Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. A corresponding resource that has been continuously updated can be found in the GitHub repository.

Transformers在自然语言处理和计算机视觉的许多任务中取得了卓越的表现，这也引起了时间序列分析领域的极大兴趣。在Transformers的多个优点中，捕捉长程依赖和交互的能力对于时间序列建模尤其具有吸引力，从而在各种时间序列应用中取得了令人兴奋的进展。在本文中，我们系统地回顾了用于时间序列建模的Transformer方案，强调了它们的优势以及局限性。特别地，我们从两个角度审视了时间序列Transformers的发展。从网络结构的角度，我们总结了为适应时间序列分析中的挑战而对Transformers进行的调整和修改。从应用的角度，我们根据常见任务（包括预测、异常检测和分类）对时间序列Transformers进行了分类。通过实证研究，我们进行了稳健性分析、模型大小分析和季节-趋势分解分析，以研究Transformers在时间序列中的表现。最后，我们讨论并提出了未来的研究方向，以提供有用的研究指导。一个持续更新的相关资源可以在GitHub仓库中找到。

## 1 Introduction

The innovation of Transformer in deep learning [Vaswani et al., 2017] has brought great interests recently due to its excellent performances in natural language processing (NLP) [Kenton and others, 2019], computer vision (CV) [Dosovitskiy et al., 2021], and speech processing [Dong et al., 2018]. Over the past few years, numerous Transformers have been proposed to advance the state-of-the-art performances of various tasks significantly. There are quite a few literature reviews from different aspects, such as in NLP applications [Han et al., 2021], CV applications [Han et al., 2022], and efficient Transformers [Tay et al., 2022].

Transformer在深度学习中的创新[Vaswani等人，2017]因其在自然语言处理(NLP)[Kenton等，2019]、计算机视觉(CV)[Dosovitskiy等人，2021]和语音处理[Dong等人，2018]中的卓越表现而最近引起了极大的兴趣。在过去的几年里，人们提出了众多Transformer以显著提升各种任务的最新性能。从不同方面已有一些文献综述，例如在NLP应用[Han等人，2021]、CV应用[Han等人，2022]以及高效Transformer[Tay等人，2022]方面的综述。

Transformers have shown great modeling ability for longrange dependencies and interactions in sequential data and thus are appealing to time series modeling. Many variants of Transformer have been proposed to address special challenges in time series modeling and have been successfully applied to various time series tasks, such as forecasting [Li et al., 2019; Zhou et al., 2022], anomaly detection [Xu et al., 2022; Tuli et al., 2022], and classification [Zerveas et al., 2021; Yang et al., 2021]. Specifically, seasonality or periodicity is an important feature of time series [Wen et al., 2021a]. How to effectively model long-range and short-range temporal dependency and capture seasonality simultaneously remains a challenge [Wu et al., 2021; Wen et al., 2022]. We note that there exist several surveys related to deep learning for time series, including forecasting [Lim and Zohren, 2021; Benidis et al., 2022; Torres et al., 2021], classification [Ismail Fawaz et al., 2019], anomaly detection [Choi et al., 2021; Bl´azquez-Garc´ıa et al., 2021], and data augmentation [Wen et al., 2021b], but there is no comprehensive survey for Transformers in time series. As Transformer for time series is an emerging subject in deep learning, a systematic and comprehensive survey on time series Transformers would greatly benefit the time series community.

Transformers在处理序列数据中的长程依赖和交互方面展现出了强大的建模能力，因此对于时间序列建模非常具有吸引力。为了解决时间序列建模中的特殊挑战，已经提出了许多Transformer变体，并成功应用于各种时间序列任务，如预测[Li等人，2019；Zhou等人，2022]、异常检测[Xu等人，2022；Tuli等人，2022]和分类[Zerveas等人，2021；Yang等人，2021]。具体来说，季节性或周期性是时间序列的一个重要特征[Wen等人，2021a]。如何同时有效地建模长程和短程的时间依赖性以及捕捉季节性仍然是一个挑战[Wu等人，2021；Wen等人，2022]。我们注意到，已经存在一些与时间序列深度学习相关的调查，包括预测[Lim和Zohren，2021；Benidis等人，2022；Torres等人，2021]、分类[Ismail Fawaz等人，2019]、异常检测[Choi等人，2021；Blázquez-García等人，2021]和数据增强[Wen等人，2021b]，但还没有针对时间序列中Transformers的全面综述。鉴于时间序列的Transformer是深度学习中的一个新兴课题，一个系统而全面的时间序列Transformers综述将极大地惠及时间序列社区。

In this paper, we aim to fill the gap by summarizing the main developments of time series Transformers. We first give a brief introduction about vanilla Transformer, and then propose a new taxonomy from perspectives of both network modifications and application domains for time series Transformers. For network modifications, we discuss the improvements made on both low-level (i.e. module) and high-level (i.e. architecture) of Transformers, with the aim to optimize the performance of time series modeling. For applications, we analyze and summarize Transformers for popular time series tasks including forecasting, anomaly detection, and classification. For each time series Transformer, we analyze its insights, strengths, and limitations. To provide practical guidelines on how to effectively use Transformers for time series modeling, we conduct extensive empirical studies that examine multiple aspects of time series modeling, including robustness analysis, model size analysis, and seasonal-trend decomposition analysis. We conclude this work by discussing possible future directions for time series Transformers, including inductive biases for time series Transformers, Transformers and GNN for time series, pre-trained Transformers for time series, Transformers with architecture level variants, and Transformers with NAS for time series. To the best of our knowledge, this is the first work to comprehensively and systematically review the key developments of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.

在本文中，我们旨在通过总结时间序列Transformers的主要发展来填补这一空白。首先，我们对原始Transformer进行简要介绍，然后从网络修改和应用领域的角度为时间序列Transformers提出一个新的分类法。对于网络修改，我们讨论了在Transformer的低级（即模块）和高级（即架构）上所做的改进，目的是优化时间序列建模的性能。对于应用领域，我们分析并总结了针对流行的时间序列任务（包括预测、异常检测和分类）的Transformers。对于每个时间序列Transformer，我们分析了其见解、优点和局限性。为了提供关于如何有效使用Transformers进行时间序列建模的实际指导，我们进行了广泛的实证研究，检查了时间序列建模的多个方面，包括稳健性分析、模型大小分析和季节-趋势分解分析。我们通过讨论时间序列Transformers的可能未来方向来结束本文的工作，包括时间序列Transformers的归纳偏置、用于时间序列的Transformers和GNN、时间序列的预训练Transformers、具有架构级别变体的Transformers以及结合NAS用于时间序列的Transformers。据我们所知，这是首个全面而系统地回顾用于建模时间序列数据的Transformers关键发展的研究。我们希望这项调查能够激发对时间序列Transformers的进一步研究兴趣。

