# A comprehensive survey of clustering algorithms: State-of-the-art machine learning applications, taxonomy, challenges, and future research prospects

## ABSTRACT

Clustering is an essential tool in data mining research and applications. It is the subject of active research in many fields of study, such as computer science, data science, statistics, pattern recognition, artificial intelligence, and machine learning. Several clustering techniques have been proposed and implemented, and most of them successfully find excellent quality or optimal clustering results in the domains mentioned earlier. However, there has been a gradual shift in the choice of clustering methods among domain experts and practitioners alike, which is precipitated by the fact that most traditional clustering algorithms still depend on the number of clusters provided a priori. These conventional clustering algorithms cannot effectively handle real-world data clustering analysis problems where the number of clusters in data objects cannot be easily identified. Also, they cannot effectively manage problems where the optimal number of clusters for a high-dimensional dataset cannot be easily determined. Therefore, there is a need for improved, flexible, and efficient clustering techniques. Recently, a variety of efficient clustering algorithms have been proposed in the literature, and these algorithms produced good results when evaluated on real-world clustering problems. This study presents an up-to-date systematic and comprehensive review of traditional and state-of-the-art clustering techniques for different domains. This survey considers clustering from a more practical perspective. It shows the outstanding role of clustering in various disciplines, such as education, marketing, medicine, biology, and bioinformatics. It also discusses the application of clustering to different fields attracting intensive efforts among the scientific community, such as big data, artificial intelligence, and robotics. This survey paper will be beneficial for both practitioners and researchers. It will serve as a good reference point for researchers and practitioners to design improved and efficient state-of-the-art clustering algorithms.

èšç±»åˆ†ææ˜¯æ•°æ®æŒ–æ˜ç ”ç©¶ä¸åº”ç”¨ä¸­çš„ä¸€é¡¹åŸºæœ¬å·¥å…·ã€‚å®ƒåœ¨è¯¸å¦‚è®¡ç®—æœºç§‘å­¦ã€æ•°æ®ç§‘å­¦ã€ç»Ÿè®¡å­¦ã€æ¨¡å¼è¯†åˆ«ã€äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ç­‰å¤šä¸ªå­¦ç§‘é¢†åŸŸå†…éƒ½æ˜¯æ´»è·ƒçš„ç ”ç©¶ä¸»é¢˜ã€‚å·²ç»æå‡ºäº†å¹¶å®ç°äº†å‡ ç§èšç±»æŠ€æœ¯ï¼Œå…¶ä¸­å¤§å¤šæ•°éƒ½èƒ½åœ¨ä¸Šè¿°é¢†åŸŸå†…æ‰¾åˆ°é«˜è´¨é‡æˆ–æœ€ä¼˜çš„èšç±»ç»“æœã€‚ç„¶è€Œï¼Œç”±äºå¤§å¤šæ•°ä¼ ç»Ÿçš„èšç±»ç®—æ³•ä»ç„¶ä¾èµ–äºäº‹å…ˆæä¾›çš„ç°‡æ•°é‡ï¼Œè¿™å¯¼è‡´äº†é¢†åŸŸä¸“å®¶å’Œä»ä¸šè€…åœ¨é€‰æ‹©èšç±»æ–¹æ³•ä¸Šé€æ¸å‘ç”Ÿäº†å˜åŒ–ã€‚è¿™äº›ä¼ ç»Ÿçš„èšç±»ç®—æ³•æ— æ³•æœ‰æ•ˆå¤„ç†å®é™…æ•°æ®ä¸­çš„èšç±»åˆ†æé—®é¢˜ï¼Œå› ä¸ºåœ¨å®é™…çš„æ•°æ®å¯¹è±¡ä¸­ï¼Œç°‡çš„æ•°é‡å¾€å¾€ä¸å®¹æ˜“ç¡®å®šã€‚è€Œä¸”ï¼Œå¯¹äºé«˜ç»´æ•°æ®é›†è€Œè¨€ï¼Œå…¶æœ€ä½³ç°‡æ•°ä¹Ÿéš¾ä»¥è½»æ˜“ç¡®å®šã€‚å› æ­¤ï¼Œéœ€è¦æ”¹è¿›çš„ã€çµæ´»é«˜æ•ˆçš„èšç±»æŠ€æœ¯ã€‚

æœ€è¿‘ï¼Œæ–‡çŒ®ä¸­æå‡ºäº†ä¸€ç³»åˆ—é«˜æ•ˆçš„èšç±»ç®—æ³•ï¼Œå¹¶ä¸”å½“å¯¹å®é™…çš„èšç±»é—®é¢˜è¿›è¡Œè¯„ä¼°æ—¶ï¼Œè¿™äº›ç®—æ³•äº§ç”Ÿäº†è‰¯å¥½çš„ç»“æœã€‚æœ¬ç ”ç©¶æä¾›äº†ä¼ ç»Ÿå’Œæœ€å…ˆè¿›èšç±»æŠ€æœ¯åœ¨ä¸åŒé¢†åŸŸçš„æœ€æ–°ç³»ç»Ÿæ€§å’Œå…¨é¢æ€§çš„ç»¼è¿°ã€‚æ­¤æ¬¡è°ƒæŸ¥ä»æ›´å®ç”¨çš„è§’åº¦è€ƒè™‘èšç±»åˆ†æï¼Œå±•ç¤ºäº†èšç±»åœ¨æ•™è‚²ã€å¸‚åœºè¥é”€ã€åŒ»å­¦ã€ç”Ÿç‰©å­¦å’Œç”Ÿç‰©ä¿¡æ¯å­¦ç­‰å­¦ç§‘ä¸­çš„çªå‡ºä½œç”¨ã€‚åŒæ—¶ï¼Œä¹Ÿè®¨è®ºäº†èšç±»åœ¨å¸å¼•ç§‘å­¦ç•Œå¯†é›†åŠªåŠ›çš„é¢†åŸŸä¸­çš„åº”ç”¨ï¼Œå¦‚å¤§æ•°æ®ã€äººå·¥æ™ºèƒ½å’Œæœºå™¨äººæŠ€æœ¯ã€‚

è¿™ç¯‡ç»¼è¿°æ–‡ç« å°†å¯¹ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜éƒ½æœ‰ç›Šï¼Œä¸ºä»–ä»¬è®¾è®¡æ”¹è¿›çš„é«˜æ•ˆæœ€å…ˆè¿›èšç±»ç®—æ³•æä¾›ä¸€ä¸ªè‰¯å¥½çš„å‚è€ƒç‚¹ã€‚

## 1. Introduction

Clustering (an aspect of data mining) is considered an active method of grouping data into many collections or clusters according to the similarities of data points features and characteristics (Jain, 2010; Abualigah, 2019). Over the past years, dozens of data clustering techniques have been proposed and implemented to solve data clustering problems (Zhou et al., 2019; Abualigah et al., 2018a,b). In general, clustering analysis techniques can be divided into two main groups: hierarchical and partitional (Tan, 2018). Although methods in these two groups have proved to be very effective and efficient, they generally depend on providing prior knowledge or information of the exact number of clusters for each dataset to be clustered and analyzed (Chang et al., 2010). More so, when dealing with real-world datasets, it is normal not to expect or have any prior information regarding the number of naturally occurring groups in the data objects (Liu et al., 2011). Therefore, the concept of automatic data clustering algorithms is introduced to address this limitation. Automatic clustering algorithms refer to any clustering techniques used to automatically determine the number of clusters without having any prior information of the dataset features and attributes (Ezugwu, 2020a). Many automatic data clustering algorithms have been proposed in the literature, and several of them are nature-inspired. The current survey presents a systematic study of traditional and recently proposed clustering techniques applied in different fields.

èšç±»åˆ†æï¼ˆæ•°æ®æŒ–æ˜çš„ä¸€ä¸ªæ–¹é¢ï¼‰è¢«è®¤ä¸ºæ˜¯æ ¹æ®æ•°æ®ç‚¹çš„ç‰¹å¾å’Œç‰¹æ€§å°†æ•°æ®åˆ†ç»„åˆ°å¤šä¸ªé›†åˆæˆ–ç°‡ä¸­çš„ç§¯ææ–¹æ³•ï¼ˆJain, 2010; Abualigah, 2019ï¼‰ã€‚åœ¨è¿‡å»çš„å‡ å¹´é‡Œï¼Œä¸ºäº†æ±‚è§£æ•°æ®èšç±»é—®é¢˜ï¼Œæå‡ºäº†å¹¶å®æ–½äº†æ•°åç§æ•°æ®èšç±»æŠ€æœ¯ï¼ˆZhouç­‰äººï¼Œ2019ï¼›Abualigahç­‰äººï¼Œ2018a,bï¼‰ã€‚é€šå¸¸æ¥è¯´ï¼Œèšç±»åˆ†ææŠ€æœ¯å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªä¸»è¦ç±»åˆ«ï¼šå±‚æ¬¡èšç±»å’Œåˆ’åˆ†èšç±»ï¼ˆTan, 2018ï¼‰ã€‚å°½ç®¡è¿™ä¸¤ç±»ä¸­çš„æ–¹æ³•å·²è¢«è¯æ˜æ˜¯éå¸¸æœ‰æ•ˆå’Œé«˜æ•ˆçš„ï¼Œä½†å®ƒä»¬é€šå¸¸ä¾èµ–äºæä¾›æ¯ä¸ªå¾…èšç±»å’Œåˆ†æçš„æ•°æ®é›†çš„ç¡®åˆ‡ç°‡æ•°ç›®çš„å…ˆéªŒçŸ¥è¯†æˆ–ä¿¡æ¯ï¼ˆChangç­‰äººï¼Œ2010ï¼‰ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨å¤„ç†ç°å®ä¸–ç•Œçš„æ•°æ®é›†æ—¶ï¼Œé€šå¸¸ä¸ä¼šæœŸæœ›æˆ–æ‹¥æœ‰å…³äºæ•°æ®å¯¹è±¡ä¸­è‡ªç„¶å‘ç”Ÿçš„ç¾¤ç»„æ•°é‡çš„ä»»ä½•å…ˆéªŒä¿¡æ¯ï¼ˆLiuç­‰äººï¼Œ2011ï¼‰ã€‚å› æ­¤ï¼Œå¼•å…¥äº†è‡ªåŠ¨æ•°æ®èšç±»ç®—æ³•çš„æ¦‚å¿µä»¥è§£å†³è¿™ä¸€é™åˆ¶ã€‚è‡ªåŠ¨èšç±»ç®—æ³•æŒ‡çš„æ˜¯ç”¨äºè‡ªåŠ¨ç¡®å®šç°‡æ•°é‡è€Œæ— éœ€äº‹å…ˆäº†è§£æ•°æ®é›†ç‰¹å¾å’Œå±æ€§çš„ä»»ä½•å…ˆéªŒä¿¡æ¯çš„ä»»ä½•èšç±»æŠ€æœ¯ï¼ˆEzugwu, 2020aï¼‰ã€‚æ–‡çŒ®ä¸­å·²ç»æå‡ºäº†è®¸å¤šè‡ªåŠ¨æ•°æ®èšç±»ç®—æ³•ï¼Œå…¶ä¸­ä¸€äº›æ˜¯å—è‡ªç„¶å¯å‘çš„ã€‚å½“å‰çš„ç»¼è¿°å¯¹åº”ç”¨äºä¸åŒé¢†åŸŸçš„ä¼ ç»Ÿå’Œæœ€è¿‘æå‡ºçš„èšç±»æŠ€æœ¯è¿›è¡Œäº†ç³»ç»Ÿçš„ç ”ç©¶ã€‚

Many surveys on clustering techniques exist in the literature (Xu and Wunsch, 2005; Xu and Tian, 2015; Benabdellah et al., 2019; Adil et al., 2014; Dafir et al., 2021; Saxena et al., 2017; Nagpal, 2013; Oyelade et al., 2016; Bindra and Mishra, 2017; Singh and Srivastava, 2020; Djouzi and Beghdad-Bey, 2019; Ezugwu, 2020a). Xu and Tian (2015) explained the basic elements involved in the clustering process and broadly categorized existing clustering algorithms into two major perspectives: the traditional and modern ones. Xu and Wunsch (2005) reviewed major clustering algorithms for datasets appearing in Statistics, Computer Science, and Machine learning. Benabdellah et al. (2019) categorized clustering algorithms using the three Vâ€™s properties of Big Data: Volume, Variety, and Velocity. These three properties were used to explore the various categories of clustering algorithms. Adil et al. (2014) gave a concise survey of existing clustering algorithms and conducted extensive experiments to highlight the best-performing clustering algorithm for Big data analysis. Berkhin et al. (2001) reviewed clustering techniques in data mining, emphasizing object attribute type, large dataset scalability, handling high dimensional data, and finding irregularly shaped clusters.

è®¸å¤šå…³äºèšç±»æŠ€æœ¯çš„ç»¼è¿°å­˜åœ¨äºæ–‡çŒ®ä¸­ï¼ˆXuå’ŒWunschï¼Œ2005ï¼›Xuå’ŒTianï¼Œ2015ï¼›Benabdellahç­‰äººï¼Œ2019ï¼›Adilç­‰äººï¼Œ2014ï¼›Dafirç­‰äººï¼Œ2021ï¼›Saxenaç­‰äººï¼Œ2017ï¼›Nagpalï¼Œ2013ï¼›Oyeladeç­‰äººï¼Œ2016ï¼›Bindraå’ŒMishraï¼Œ2017ï¼›Singhå’ŒSrivastavaï¼Œ2020ï¼›Djouziå’ŒBeghdad-Beyï¼Œ2019ï¼›Ezugwuï¼Œ2020aï¼‰ã€‚Xuå’ŒTianï¼ˆ2015ï¼‰è§£é‡Šäº†èšç±»è¿‡ç¨‹ä¸­æ¶‰åŠçš„åŸºæœ¬å…ƒç´ ï¼Œå¹¶å°†ç°æœ‰çš„èšç±»ç®—æ³•å¹¿æ³›åˆ†ç±»ä¸ºä¸¤ä¸ªä¸»è¦è§†è§’ï¼šä¼ ç»Ÿç®—æ³•å’Œç°ä»£ç®—æ³•ã€‚Xuå’ŒWunschï¼ˆ2005ï¼‰å›é¡¾äº†åœ¨ç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¸­å‡ºç°çš„æ•°æ®é›†çš„ä¸»è¦èšç±»ç®—æ³•ã€‚Benabdellahç­‰äººï¼ˆ2019ï¼‰ä½¿ç”¨å¤§æ•°æ®çš„ä¸‰ä¸ªVç‰¹æ€§â€”â€”ä½“é‡ï¼ˆVolumeï¼‰ã€å¤šæ ·æ€§ï¼ˆVarietyï¼‰å’Œé€Ÿåº¦ï¼ˆVelocityï¼‰å¯¹èšç±»ç®—æ³•è¿›è¡Œäº†åˆ†ç±»ï¼Œè¿™ä¸‰ä¸ªç‰¹æ€§è¢«ç”¨æ¥æ¢ç´¢èšç±»ç®—æ³•çš„å„ç§ç±»åˆ«ã€‚Adilç­‰äººï¼ˆ2014ï¼‰å¯¹ç°æœ‰èšç±»ç®—æ³•è¿›è¡Œäº†ç®€æ˜çš„è°ƒæŸ¥ï¼Œå¹¶è¿›è¡Œäº†å¹¿æ³›çš„å®éªŒä»¥çªå‡ºè¡¨ç°æœ€ä½³çš„å¤§æ•°æ®åˆ†æèšç±»ç®—æ³•ã€‚Berkhinç­‰äººï¼ˆ2001ï¼‰å›é¡¾äº†æ•°æ®æŒ–æ˜ä¸­çš„èšç±»æŠ€æœ¯ï¼Œå¼ºè°ƒäº†å¯¹è±¡å±æ€§ç±»å‹ã€å¤§è§„æ¨¡æ•°æ®é›†çš„å¯æ‰©å±•æ€§ã€å¤„ç†é«˜ç»´æ•°æ®ä»¥åŠå‘ç°ä¸è§„åˆ™å½¢çŠ¶çš„ç°‡ã€‚

Dafir et al. (2021)â€™s work was on parallel clustering algorithms, classifying and summarizing them. He discussed the framework for each kind of parallel clustering algorithm. Saxena et al. (2017) presented a taxonomy of existing clustering algorithms, debating each algorithmâ€™s various measures of similarity and evaluation criteria. Nagpal (2013) carried out a comparative analysis of the different clustering algorithms concerning both the mixed and categorical datasets with the observation that no clustering algorithm can be adjudged as best for handling a large dataset of either the mixed or categorical dataset. Oyelade et al. (2016) examined various clustering algorithms and their suitability for gene expression data to discover and provide helpful knowledge that will guarantee stability and a high degree of accuracy in the area. Jain (2010) summarized well-known clustering methods with a discussion on critical issues and challenges in the design of clustering algorithms. Jain et al. (1999) discussed emerging techniques for non-numeric constraints and large sets of patterns. Ezugwu et al. (2020a) presented an in-depth and systematic review of nature-inspired metaheuristic algorithms used for automatic clustering analysis focusing on the metaheuristic algorithms that have been employed to solve clustering problems over the last three decades.

Dafirç­‰äººï¼ˆ2021ï¼‰çš„ç ”ç©¶èšç„¦äºå¹¶è¡Œèšç±»ç®—æ³•ï¼Œå¯¹ä»–ä»¬è¿›è¡Œäº†åˆ†ç±»å’Œæ€»ç»“ï¼Œå¹¶è®¨è®ºäº†æ¯ç§å¹¶è¡Œèšç±»ç®—æ³•çš„æ¡†æ¶ã€‚Saxenaç­‰äººï¼ˆ2017ï¼‰æå‡ºäº†ç°æœ‰èšç±»ç®—æ³•çš„åˆ†ç±»ï¼Œè¾©è®ºäº†æ¯ä¸ªç®—æ³•çš„å„ç§ç›¸ä¼¼æ€§åº¦é‡å’Œè¯„ä¼°æ ‡å‡†ã€‚Nagpalï¼ˆ2013ï¼‰å¯¹ä¸åŒèšç±»ç®—æ³•è¿›è¡Œäº†æ¯”è¾ƒåˆ†æï¼Œæ¶‰åŠæ··åˆæ•°æ®é›†å’Œåˆ†ç±»æ•°æ®é›†ï¼Œè§‚å¯Ÿåˆ°æ²¡æœ‰ä¸€ç§èšç±»ç®—æ³•å¯ä»¥è¢«åˆ¤å®šä¸ºæœ€é€‚åˆå¤„ç†å¤§è§„æ¨¡çš„æ··åˆæˆ–åˆ†ç±»æ•°æ®é›†ã€‚Oyeladeç­‰äººï¼ˆ2016ï¼‰æ£€æŸ¥äº†å„ç§èšç±»ç®—æ³•åŠå…¶å¯¹åŸºå› è¡¨è¾¾æ•°æ®çš„é€‚ç”¨æ€§ï¼Œä»¥å‘ç°å¹¶æä¾›æœ‰åŠ©äºä¿è¯è¯¥é¢†åŸŸç¨³å®šæ€§å’Œé«˜å‡†ç¡®åº¦çš„æœ‰ç”¨çŸ¥è¯†ã€‚Jainï¼ˆ2010ï¼‰æ€»ç»“äº†çŸ¥åçš„èšç±»æ–¹æ³•ï¼Œå¹¶è®¨è®ºäº†è®¾è®¡èšç±»ç®—æ³•æ—¶çš„å…³é”®é—®é¢˜å’ŒæŒ‘æˆ˜ã€‚Jainç­‰äººï¼ˆ1999ï¼‰æ¢è®¨äº†é’ˆå¯¹éæ•°å€¼çº¦æŸå’Œå¤§å‹æ¨¡å¼é›†çš„æ–°æŠ€æœ¯ã€‚Ezugwuç­‰äººï¼ˆ2020aï¼‰æ·±å…¥ç³»ç»Ÿåœ°å›é¡¾äº†è¿‡å»ä¸‰åå¹´ä¸­ç”¨äºè‡ªåŠ¨èšç±»åˆ†æçš„è‡ªç„¶å¯å‘å¼å…ƒå¯å‘ç®—æ³•ï¼Œé‡ç‚¹å…³æ³¨é‚£äº›å·²è¢«ç”¨æ¥è§£å†³èšç±»é—®é¢˜çš„å…ƒå¯å‘ç®—æ³•ã€‚

#### 4.2.1.6 Square error clustering 

The square error clustering method is a partitioning clustering method that assigns data points into a specified number of clusters based on the sum of square error criterion functions. The squared differences between each data point and the estimated center value for each stated group have been divided into the data point. In cases where the sum of squared error for a group of data objects is equal to zero, the clusterâ€™s data points are identical (very close). The formula for Sum of Square Error is:

$$\text{ğ‘†ğ‘¢ğ‘š ğ‘œğ‘“ ğ‘ ğ‘ğ‘¢ğ‘ğ‘Ÿğ‘’ ğ‘’ğ‘Ÿğ‘Ÿğ‘œğ‘Ÿ} = \Sigma_{ğ‘–=1}^n (ğ‘¥ğ‘– âˆ’ ğ‘¥)^2$$

where ğ‘› represents the number of data points and ğ‘¥i represents the ğ‘–th data point in the group and ğ‘¥ is the center object relative to the group. The k-means clustering algorithm is the best know squared error-based clustering algorithm (Xu and Wunsch, 2005).

##### i. K-Means Clustering

The K-Means clustering algorithm is a centroid-based partitioning technique in which data objects are distributed into a specified number of k clusters. The distribution is done through the use of an objective function which accesses the quality of the partition, ensuring that the similarities of objects within a cluster (intra-cluster similarity) is higher compared with objects in another cluster (inter-cluster similarity). KMeans clustering is a centroid-based technique, and it uses the mean to represent the centroid of a cluster. The centroid of a cluster is a measure of the center point of the cluster. Specified ğ‘˜ numbers of data points/objects are randomly selected from a set of the existing data points as the representative center for ğ‘˜ clusters. The Euclidean distance between the remaining data points and each assumed center point is then iteratively measured. The obtained value assigns the data point to the cluster with the smallest distance. The intracluster similarity is improved each time a new data point is given to the cluster by computing a new mean using the objects previously assigned to the clusters. The new mean is then used to reassign the data objects. This procedure is repeated severally until stability is achieved.

K-Meansèšç±»ç®—æ³•æ˜¯ä¸€ç§åŸºäºè´¨å¿ƒçš„åˆ’åˆ†æŠ€æœ¯ï¼Œå…¶ä¸­æ•°æ®å¯¹è±¡è¢«åˆ†é…åˆ°æŒ‡å®šæ•°é‡çš„kä¸ªç°‡ä¸­ã€‚åˆ†é…è¿‡ç¨‹æ˜¯é€šè¿‡ä½¿ç”¨ä¸€ä¸ªç›®æ ‡å‡½æ•°æ¥å®Œæˆçš„ï¼Œè¯¥å‡½æ•°è¯„ä¼°åˆ†åŒºçš„è´¨é‡ï¼Œç¡®ä¿åŒä¸€ç°‡å†…å¯¹è±¡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆç°‡å†…ç›¸ä¼¼æ€§ï¼‰é«˜äºå…¶ä»–ç°‡ä¸­çš„å¯¹è±¡ï¼ˆç°‡é—´ç›¸ä¼¼æ€§ï¼‰ã€‚KMeansèšç±»æ˜¯ä¸€ç§åŸºäºè´¨å¿ƒçš„æŠ€æœ¯ï¼Œå®ƒä½¿ç”¨å¹³å‡å€¼æ¥è¡¨ç¤ºç°‡çš„è´¨å¿ƒã€‚ç°‡çš„è´¨å¿ƒæ˜¯è¯¥ç°‡ä¸­å¿ƒç‚¹çš„ä¸€ç§åº¦é‡ã€‚ä»ç°æœ‰çš„æ•°æ®ç‚¹é›†ä¸­éšæœºé€‰æ‹©æŒ‡å®šæ•°é‡ğ‘˜çš„æ•°æ®ç‚¹/å¯¹è±¡ä½œä¸ºğ‘˜ä¸ªç°‡çš„ä»£è¡¨æ€§ä¸­å¿ƒã€‚ç„¶åè¿­ä»£åœ°æµ‹é‡å‰©ä½™æ•°æ®ç‚¹ä¸æ¯ä¸ªå‡å®šä¸­å¿ƒç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»ã€‚è·å¾—çš„å€¼å°†æ•°æ®ç‚¹åˆ†é…ç»™è·ç¦»æœ€å°çš„ç°‡ã€‚æ¯å½“æœ‰æ–°çš„æ•°æ®ç‚¹è¢«åˆ†é…ç»™ç°‡æ—¶ï¼Œé€šè¿‡è®¡ç®—ä¹‹å‰åˆ†é…ç»™ç°‡çš„å¯¹è±¡çš„æ–°å‡å€¼æ¥æå‡ç°‡å†…ç›¸ä¼¼åº¦ã€‚æ–°å‡å€¼éšåç”¨äºé‡æ–°åˆ†é…æ•°æ®å¯¹è±¡ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤å¤šæ¬¡ï¼Œç›´åˆ°è¾¾åˆ°ç¨³å®šçŠ¶æ€ã€‚

The sum of square function for the Euclidean distances produces compact and well-separated clusters. The K-Means algorithm tries to minimize the sum of the squared error criterion (Ezugwu, 2020a; Hartigan and Wong, 1979; MacQueen, 1967). The major problems identified with K-Means clustering algorithms include the problem of the initial definition of the number of clusters at the algorithmâ€™s onset. An efficient and universal method for determining the initial number of clusters and partition is not found. K-Means algorithm is reported to be very sensitive to initial centroid selection such that suboptimal solution may be produced when wrongfully chosen (Punit, 2018). Also, convergence to global optimum cannot be guaranteed. Using means as a centroid limits the K-Means algorithmâ€™s application to data objects with numerical variables (Xu and Wunsch, 2005). Not only these, but the K-Means algorithm is also sensitive to outliers (objects that are quite far from the cluster centroid are forced into the cluster, distorting the clusterâ€™s shape (Saxena et al., 2017). It works on the assumption that the variance of the distribution of each attribute is spherical and thus produces a roughly equal number of observations. Moreover, the memory space requirement is high and the number of iterations to obtain a stable distribution is unknown. Due to the simplicity of implementation and low computation complexity (Jain, 2010), the K-Means algorithm is still popular and widely used today (Ezugwu, 2020a).

å¹³æ–¹å’Œå‡½æ•°åœ¨æ¬§æ°è·ç¦»ä¸‹èƒ½ç”Ÿæˆç´§å‡‘ä¸”åˆ†ç¦»è‰¯å¥½çš„ç°‡ã€‚K å‡å€¼ç®—æ³•è¯•å›¾æœ€å°åŒ–å¹³æ–¹è¯¯å·®å‡†åˆ™ï¼ˆEzugwu, 2020a; Hartigan and Wong, 1979; MacQueen, 1967ï¼‰ã€‚è¯¥ç®—æ³•å­˜åœ¨çš„ä¸»è¦é—®é¢˜åŒ…æ‹¬ï¼šéœ€åœ¨ç®—æ³•å¯åŠ¨æ—¶é¢„å…ˆå®šä¹‰ç°‡çš„æ•°é‡ã€‚ç›®å‰å°šæœªå‘ç°ä¸€ç§é«˜æ•ˆä¸”é€šç”¨çš„æ–¹æ³•æ¥ç¡®å®šåˆå§‹ç°‡æ•°å’Œåˆ’åˆ†æ–¹å¼ã€‚å·²æœ‰ç ”ç©¶è¡¨æ˜ï¼ŒK å‡å€¼ç®—æ³•å¯¹åˆå§‹è´¨å¿ƒçš„é€‰æ‹©æä¸ºæ•æ„Ÿï¼Œé”™è¯¯çš„è´¨å¿ƒé€‰æ‹©å¯èƒ½å¯¼è‡´æ¬¡ä¼˜è§£ï¼ˆPunit, 2018ï¼‰ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•æ— æ³•ä¿è¯æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚ä½¿ç”¨å‡å€¼ä½œä¸ºè´¨å¿ƒçš„æ–¹å¼ï¼Œå°† K å‡å€¼ç®—æ³•çš„åº”ç”¨é™åˆ¶åœ¨å…·æœ‰æ•°å€¼å‹å˜é‡çš„æ•°æ®å¯¹è±¡ä¸Šï¼ˆXu and Wunsch, 2005ï¼‰ã€‚ä¸ä»…å¦‚æ­¤ï¼ŒK å‡å€¼ç®—æ³•å¯¹ç¦»ç¾¤ç‚¹ä¹Ÿéå¸¸æ•æ„Ÿ â€”â€” é‚£äº›è·ç¦»ç°‡å¿ƒè¾ƒè¿œçš„å¯¹è±¡ä¼šè¢«å¼ºè¡Œå½’å…¥è¯¥ç°‡ï¼Œä»è€Œæ‰­æ›²ç°‡çš„å½¢çŠ¶ï¼ˆSaxena et al., 2017ï¼‰ã€‚è¯¥ç®—æ³•åŸºäºå„å±æ€§åˆ†å¸ƒæ–¹å·®å‘ˆçƒå½¢çš„å‡è®¾ï¼Œå› æ­¤ç”Ÿæˆçš„è§‚æµ‹æ•°é‡å¤§è‡´ç›¸ç­‰ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•è¿˜å­˜åœ¨å†…å­˜ç©ºé—´éœ€æ±‚é«˜ã€è·å¾—ç¨³å®šåˆ†å¸ƒæ‰€éœ€è¿­ä»£æ¬¡æ•°æœªçŸ¥ç­‰é—®é¢˜ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç”±äºå…¶å®ç°ç®€å•ä¸”è®¡ç®—å¤æ‚åº¦ä½ï¼ˆJain, 2010ï¼‰ï¼ŒK å‡å€¼ç®—æ³•è‡³ä»Šä»è¢«å¹¿æ³›ä½¿ç”¨ï¼ˆEzugwu, 2020aï¼‰ã€‚

Some research work extending K-Means has been reported. For
example, the G-means (Hamerly and Elkan, 2004) and the X-means
algorithms (Pelleg, 2000). The sum of square function for the Euclidean
distances for the K-Means algorithm is given as:

$$ğ‘‘_{ğ‘–ğ‘˜} =\Sigma^ğ‘š_{ğ‘—=1}(ğ‘¥_{ğ‘–ğ‘—} âˆ’ ğ‘{ğ‘˜ğ‘—})^2$$

The K-Means is arguably the most popular clustering method but is plagued with drawbacks such as poor scalability, sensitivity to initialization and outliers, assumed knowledge of cluster count, and local production rather than the global optimum. It is noteworthy to mention that the most recent extensions and improvements on the K-Means seek to advance the state-of-the-art in addressing these issues.

K å‡å€¼ç®—æ³•æ— ç–‘æ˜¯æœ€å—æ¬¢è¿çš„èšç±»æ–¹æ³•ï¼Œä½†å­˜åœ¨è¯¸å¤šç¼ºç‚¹ï¼Œä¾‹å¦‚å¯æ‰©å±•æ€§å·®ã€å¯¹åˆå§‹åŒ–å’Œç¦»ç¾¤ç‚¹æ•æ„Ÿã€éœ€è¦é¢„å…ˆçŸ¥é“ç°‡æ•°ï¼Œä»¥åŠå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜è€Œéå…¨å±€æœ€ä¼˜è§£ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿‘æœŸé’ˆå¯¹ K å‡å€¼ç®—æ³•çš„æ‰©å±•å’Œæ”¹è¿›ç ”ç©¶æ­£è‡´åŠ›äºåœ¨è§£å†³è¿™äº›é—®é¢˜ä¸Šæ¨åŠ¨æŠ€æœ¯å‰æ²¿çš„å‘å±•ã€‚

##### ii. K-MCI (K-Means modified cohort intelligence) Clustering algorithm

The K-MCI (K-Means modified cohort intelligence) is an efficient hybrid evolutionary data clustering algorithm that combines the KMeans algorithm with modified cohort intelligence (Krishnasamy et al., 2014). Cohort Intelligence(CI) is an optimization algorithm inspired by the natural and societal tendency of cohort candidates/individuals learning from one another. It was proposed by Kulkarni et al. (2013). In cohort intelligence, while observing every other candidate, each candidate tries to improve their behavior. The MCI is a modified cohort intelligence with improved accuracy and speed of convergence of the traditional CI. In K-MCI, the K-Means algorithm enhances the candidateâ€™s behavior generated by MCI, annexing the advantages of the K-Means algorithm and that of the MCI. K-MCI converges more quickly with greater clustering accuracy without being trapped in the local optimum.

K-MCIï¼ˆK å‡å€¼æ”¹è¿›é˜Ÿåˆ—æ™ºèƒ½ï¼‰æ˜¯ä¸€ç§é«˜æ•ˆçš„æ··åˆè¿›åŒ–æ•°æ®èšç±»ç®—æ³•ï¼Œå®ƒå°† K å‡å€¼ç®—æ³•ä¸æ”¹è¿›çš„é˜Ÿåˆ—æ™ºèƒ½ï¼ˆMCIï¼‰ç›¸ç»“åˆï¼ˆKrishnasamy ç­‰ï¼Œ2014ï¼‰ã€‚é˜Ÿåˆ—æ™ºèƒ½ï¼ˆCIï¼‰æ˜¯ç”± Kulkarni ç­‰ï¼ˆ2013ï¼‰æå‡ºçš„ä¸€ç§å—ç¾¤ä½“å€™é€‰è€… / ä¸ªä½“ç›¸äº’å­¦ä¹ çš„è‡ªç„¶ä¸ç¤¾ä¼šè¡Œä¸ºå¯å‘çš„ä¼˜åŒ–ç®—æ³•ã€‚åœ¨é˜Ÿåˆ—æ™ºèƒ½ä¸­ï¼Œæ¯ä¸ªå€™é€‰è€…é€šè¿‡è§‚å¯Ÿå…¶ä»–å€™é€‰è€…æ¥å°è¯•æ”¹è¿›è‡ªèº«è¡Œä¸ºã€‚MCI ä½œä¸ºæ”¹è¿›çš„é˜Ÿåˆ—æ™ºèƒ½ç®—æ³•ï¼Œåœ¨ä¼ ç»Ÿ CI åŸºç¡€ä¸Šæå‡äº†æ”¶æ•›é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚K-MCI é€šè¿‡ K å‡å€¼ç®—æ³•å¢å¼ºäº† MCI ç”Ÿæˆçš„å€™é€‰è¡Œä¸ºï¼Œèåˆäº†ä¸¤ç§ç®—æ³•çš„ä¼˜åŠ¿ã€‚è¯¥ç®—æ³•ä¸ä»…æ”¶æ•›é€Ÿåº¦æ›´å¿«ã€èšç±»ç²¾åº¦æ›´é«˜ï¼Œè¿˜èƒ½æœ‰æ•ˆé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚

##### iii. ELM K-Means (Extreme Learning Machine K-Means)

In ELM K-Means, the extreme learning machine (ELM) method is incorporated into the K-Means clustering algorithm (Alshamiri et al., 2015). The ELM method functions in projecting the dataset into a high dimensional feature space, and the K-Means algorithm is used to cluster the dataset using the Euclidean distance in the feature space to measure the similarity between the objects. The ELM proposed by Huang et al. (2006) is a new learning algorithm that randomly generates hidden nodes for single hidden layer feedforward neural networks(SLFNs) and determines the output weight of the SLFNs analytically. ELM is credited with a meager computational cost for its operations and has been used in finding the solution to classification and regression problems.

åœ¨ ELM K-Means ç®—æ³•ä¸­ï¼Œæé™å­¦ä¹ æœºï¼ˆELMï¼‰æ–¹æ³•è¢«èå…¥ K å‡å€¼èšç±»ç®—æ³•ï¼ˆAlshamiri ç­‰ï¼Œ2015ï¼‰ã€‚ELM æ–¹æ³•çš„ä½œç”¨æ˜¯å°†æ•°æ®é›†æŠ•å½±åˆ°é«˜ç»´ç‰¹å¾ç©ºé—´ï¼Œéšå K å‡å€¼ç®—æ³•åˆ©ç”¨è¯¥ç‰¹å¾ç©ºé—´ä¸­çš„æ¬§æ°è·ç¦»å¯¹æ•°æ®å¯¹è±¡è¿›è¡Œç›¸ä¼¼åº¦åº¦é‡å¹¶å®ç°èšç±»ã€‚ç”± Huang ç­‰ï¼ˆ2006ï¼‰æå‡ºçš„ ELM æ˜¯ä¸€ç§æ–°å‹å­¦ä¹ ç®—æ³•ï¼Œå…¶é€šè¿‡éšæœºç”Ÿæˆå•éšå±‚å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆSLFNsï¼‰çš„éšå±‚èŠ‚ç‚¹ï¼Œå¹¶ä»¥è§£ææ–¹å¼ç¡®å®šç½‘ç»œçš„è¾“å‡ºæƒé‡ã€‚ELM å› å…¶è¿ç®—çš„è®¡ç®—æˆæœ¬æä½è€Œå¹¿å—è®¤å¯ï¼Œå·²è¢«ç”¨äºè§£å†³åˆ†ç±»å’Œå›å½’é—®é¢˜ã€‚

##### iv. K-means based multiview clustering methods and K-means subspace clustering models

The generation of high dimensional data due to the social networkâ€™s rapid development has been a significant challenge to the traditional K-means clustering generally tagged as the curse of dimensionality. Redundant features and noises in such data make efficient clustering of such data very difficult. The K-means based multiview clustering methods are developed to provide simple and efficient algorithms for accurately exploring shared information in multiview data. Zheng et al. (2018) proposed a robust discriminative multiview K-means clustering with feature selection and group sparsity learning. The proposed algorithm addressed the problem of extreme time consuming and sensitivity to outliers that is common with clustering of high-dimensional feature space. It efficiently handles the curse of dimensionality by using group sparsity constraints for selecting the most important views and the most relevant features.

ç”±äºç¤¾äº¤ç½‘ç»œçš„å¿«é€Ÿå‘å±•è€Œäº§ç”Ÿçš„é«˜ç»´æ•°æ®å¯¹ä¼ ç»Ÿçš„Kå‡å€¼èšç±»æ„æˆäº†é‡å¤§æŒ‘æˆ˜ï¼Œè¿™é€šå¸¸è¢«ç§°ä¸ºâ€œç»´åº¦ç¾éš¾â€ã€‚æ­¤ç±»æ•°æ®ä¸­çš„å†—ä½™ç‰¹å¾å’Œå™ªå£°ä½¿å¾—å¯¹å…¶è¿›è¡Œé«˜æ•ˆèšç±»å˜å¾—éå¸¸å›°éš¾ã€‚åŸºäºKå‡å€¼çš„å¤šè§†å›¾èšç±»æ–¹æ³•è¢«å¼€å‘å‡ºæ¥ï¼Œæ—¨åœ¨ä¸ºç²¾ç¡®æ¢ç´¢å¤šè§†å›¾æ•°æ®ä¸­çš„å…±äº«ä¿¡æ¯æä¾›ç®€å•è€Œé«˜æ•ˆçš„ç®—æ³•ã€‚Zhengç­‰äººï¼ˆ2018ï¼‰æå‡ºäº†ä¸€ç§å…·æœ‰ç‰¹å¾é€‰æ‹©å’Œç»„ç¨€ç–æ€§å­¦ä¹ çš„é²æ£’åˆ¤åˆ«å¤šè§†å›¾Kå‡å€¼èšç±»ç®—æ³•ã€‚è¯¥ç®—æ³•è§£å†³äº†åœ¨é«˜ç»´ç‰¹å¾ç©ºé—´èšç±»ä¸­å¸¸è§çš„æç«¯è€—æ—¶å’Œå¯¹å¼‚å¸¸å€¼æ•æ„Ÿçš„é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨ç»„ç¨€ç–æ€§çº¦æŸæ¥é€‰æ‹©æœ€é‡è¦çš„è§†å›¾å’Œæœ€ç›¸å…³çš„ç‰¹å¾ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåº”å¯¹ç»´åº¦ç¾éš¾ã€‚

In handling high dimensional data of a real-world application, using eigenvalue decomposition by existing K-means subspace clustering algorithm to find an approximate solution is less efficient. Moreover, their loss functions exhibit sensitivity to outliers or suffer small loss errors (Wang et al., 2019). A new adaptive Multiview subspace clustering method was recently proposed by Yan et al. (2020) for integrating heterogeneous data in low-dimensional feature space. Their work extended K-Means clustering with feature learning capability for handling high-dimensional data. Wang et al. (2019) developed a fast adaptive K-means (FAKM) type subspace clustering model embedded with a mechanism for flexible cluster indicator using an adaptive loss function.

åœ¨å¤„ç†å®é™…åº”ç”¨ä¸­çš„é«˜ç»´æ•°æ®æ—¶ï¼Œä½¿ç”¨ç°æœ‰çš„Kå‡å€¼å­ç©ºé—´èšç±»ç®—æ³•é€šè¿‡ç‰¹å¾å€¼åˆ†è§£æ‰¾åˆ°è¿‘ä¼¼è§£çš„æ•ˆç‡è¾ƒä½ã€‚æ­¤å¤–ï¼Œå®ƒä»¬çš„æŸå¤±å‡½æ•°å¯¹å¤–å¼‚å¸¸å€¼æ•æ„Ÿæˆ–å­˜åœ¨è¾ƒå°çš„æŸå¤±è¯¯å·®ï¼ˆWangç­‰äººï¼Œ2019ï¼‰ã€‚æœ€è¿‘ï¼ŒYanç­‰äººï¼ˆ2020ï¼‰æå‡ºäº†ä¸€ç§æ–°çš„è‡ªé€‚åº”å¤šè§†å›¾å­ç©ºé—´èšç±»æ–¹æ³•ï¼Œç”¨äºåœ¨ä½ç»´ç‰¹å¾ç©ºé—´ä¸­æ•´åˆå¼‚æ„æ•°æ®ã€‚ä»–ä»¬çš„å·¥ä½œæ‰©å±•äº†Kå‡å€¼èšç±»ï¼Œå¢åŠ äº†ç‰¹å¾å­¦ä¹ èƒ½åŠ›ï¼Œä»¥å¤„ç†é«˜ç»´æ•°æ®ã€‚Wangç­‰äººï¼ˆ2019ï¼‰å¼€å‘äº†ä¸€ç§å¿«é€Ÿè‡ªé€‚åº”Kå‡å€¼ï¼ˆFAKMï¼‰ç±»å‹çš„å­ç©ºé—´èšç±»æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åµŒå…¥äº†ä¸€ä¸ªä½¿ç”¨è‡ªé€‚åº”æŸå¤±å‡½æ•°çš„çµæ´»èšç±»æŒ‡ç¤ºæœºåˆ¶ã€‚

According to Wang et al. the existing methods of combining subspace learning with K-means clustering still exhibit some limitations. These include: no thorough capturing of discriminative information in low-dimensional subspace, consideration of intrinsic geometric information is rare, and the vulnerability to noises of the optimizing procedure of a discrete cluster indicator. They proposed a robust dimension reduction for clustering with a local adaptive learning algorithm to address these limitations. The proposed algorithm adaptively explores the discriminative information by unifying K-means clustering with local adaptive subspace learning.

æ®Wangç­‰äººæ‰€è¿°ï¼Œç°æœ‰çš„å°†å­ç©ºé—´å­¦ä¹ ä¸Kå‡å€¼èšç±»ç›¸ç»“åˆçš„æ–¹æ³•ä»ç„¶å­˜åœ¨ä¸€äº›å±€é™æ€§ã€‚è¿™äº›å±€é™åŒ…æ‹¬ï¼šæœªèƒ½åœ¨ä½ç»´å­ç©ºé—´ä¸­å½»åº•æ•è·åˆ¤åˆ«ä¿¡æ¯ï¼Œå¾ˆå°‘è€ƒè™‘å†…åœ¨å‡ ä½•ä¿¡æ¯ï¼Œä»¥åŠç¦»æ•£èšç±»æŒ‡ç¤ºä¼˜åŒ–è¿‡ç¨‹æ˜“å—å™ªå£°å½±å“ã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä»–ä»¬æå‡ºäº†ä¸€ç§ç»“åˆå±€éƒ¨è‡ªé€‚åº”å­¦ä¹ ç®—æ³•çš„é²æ£’é™ç»´èšç±»æ–¹æ³•ã€‚æ‰€æå‡ºçš„ç®—æ³•é€šè¿‡å°†Kå‡å€¼èšç±»ä¸å±€éƒ¨è‡ªé€‚åº”å­ç©ºé—´å­¦ä¹ ç»Ÿä¸€èµ·æ¥ï¼Œè‡ªé€‚åº”åœ°æ¢ç´¢åˆ¤åˆ«ä¿¡æ¯ã€‚

