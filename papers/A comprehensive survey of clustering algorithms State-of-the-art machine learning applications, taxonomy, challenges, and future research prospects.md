# A comprehensive survey of clustering algorithms: State-of-the-art machine learning applications, taxonomy, challenges, and future research prospects

## ABSTRACT

Clustering is an essential tool in data mining research and applications. It is the subject of active research in many fields of study, such as computer science, data science, statistics, pattern recognition, artificial intelligence, and machine learning. Several clustering techniques have been proposed and implemented, and most of them successfully find excellent quality or optimal clustering results in the domains mentioned earlier. However, there has been a gradual shift in the choice of clustering methods among domain experts and practitioners alike, which is precipitated by the fact that most traditional clustering algorithms still depend on the number of clusters provided a priori. These conventional clustering algorithms cannot effectively handle real-world data clustering analysis problems where the number of clusters in data objects cannot be easily identified. Also, they cannot effectively manage problems where the optimal number of clusters for a high-dimensional dataset cannot be easily determined. Therefore, there is a need for improved, flexible, and efficient clustering techniques. Recently, a variety of efficient clustering algorithms have been proposed in the literature, and these algorithms produced good results when evaluated on real-world clustering problems. This study presents an up-to-date systematic and comprehensive review of traditional and state-of-the-art clustering techniques for different domains. This survey considers clustering from a more practical perspective. It shows the outstanding role of clustering in various disciplines, such as education, marketing, medicine, biology, and bioinformatics. It also discusses the application of clustering to different fields attracting intensive efforts among the scientific community, such as big data, artificial intelligence, and robotics. This survey paper will be beneficial for both practitioners and researchers. It will serve as a good reference point for researchers and practitioners to design improved and efficient state-of-the-art clustering algorithms.

聚类分析是数据挖掘研究与应用中的一项基本工具。它在诸如计算机科学、数据科学、统计学、模式识别、人工智能和机器学习等多个学科领域内都是活跃的研究主题。已经提出了并实现了几种聚类技术，其中大多数都能在上述领域内找到高质量或最优的聚类结果。然而，由于大多数传统的聚类算法仍然依赖于事先提供的簇数量，这导致了领域专家和从业者在选择聚类方法上逐渐发生了变化。这些传统的聚类算法无法有效处理实际数据中的聚类分析问题，因为在实际的数据对象中，簇的数量往往不容易确定。而且，对于高维数据集而言，其最佳簇数也难以轻易确定。因此，需要改进的、灵活高效的聚类技术。

最近，文献中提出了一系列高效的聚类算法，并且当对实际的聚类问题进行评估时，这些算法产生了良好的结果。本研究提供了传统和最先进聚类技术在不同领域的最新系统性和全面性的综述。此次调查从更实用的角度考虑聚类分析，展示了聚类在教育、市场营销、医学、生物学和生物信息学等学科中的突出作用。同时，也讨论了聚类在吸引科学界密集努力的领域中的应用，如大数据、人工智能和机器人技术。

这篇综述文章将对从业者和研究人员都有益，为他们设计改进的高效最先进聚类算法提供一个良好的参考点。

## 1. Introduction

Clustering (an aspect of data mining) is considered an active method of grouping data into many collections or clusters according to the similarities of data points features and characteristics (Jain, 2010; Abualigah, 2019). Over the past years, dozens of data clustering techniques have been proposed and implemented to solve data clustering problems (Zhou et al., 2019; Abualigah et al., 2018a,b). In general, clustering analysis techniques can be divided into two main groups: hierarchical and partitional (Tan, 2018). Although methods in these two groups have proved to be very effective and efficient, they generally depend on providing prior knowledge or information of the exact number of clusters for each dataset to be clustered and analyzed (Chang et al., 2010). More so, when dealing with real-world datasets, it is normal not to expect or have any prior information regarding the number of naturally occurring groups in the data objects (Liu et al., 2011). Therefore, the concept of automatic data clustering algorithms is introduced to address this limitation. Automatic clustering algorithms refer to any clustering techniques used to automatically determine the number of clusters without having any prior information of the dataset features and attributes (Ezugwu, 2020a). Many automatic data clustering algorithms have been proposed in the literature, and several of them are nature-inspired. The current survey presents a systematic study of traditional and recently proposed clustering techniques applied in different fields.

聚类分析（数据挖掘的一个方面）被认为是根据数据点的特征和特性将数据分组到多个集合或簇中的积极方法（Jain, 2010; Abualigah, 2019）。在过去的几年里，为了求解数据聚类问题，提出了并实施了数十种数据聚类技术（Zhou等人，2019；Abualigah等人，2018a,b）。通常来说，聚类分析技术可以分为两个主要类别：层次聚类和划分聚类（Tan, 2018）。尽管这两类中的方法已被证明是非常有效和高效的，但它们通常依赖于提供每个待聚类和分析的数据集的确切簇数目的先验知识或信息（Chang等人，2010）。更重要的是，在处理现实世界的数据集时，通常不会期望或拥有关于数据对象中自然发生的群组数量的任何先验信息（Liu等人，2011）。因此，引入了自动数据聚类算法的概念以解决这一限制。自动聚类算法指的是用于自动确定簇数量而无需事先了解数据集特征和属性的任何先验信息的任何聚类技术（Ezugwu, 2020a）。文献中已经提出了许多自动数据聚类算法，其中一些是受自然启发的。当前的综述对应用于不同领域的传统和最近提出的聚类技术进行了系统的研究。

Many surveys on clustering techniques exist in the literature (Xu and Wunsch, 2005; Xu and Tian, 2015; Benabdellah et al., 2019; Adil et al., 2014; Dafir et al., 2021; Saxena et al., 2017; Nagpal, 2013; Oyelade et al., 2016; Bindra and Mishra, 2017; Singh and Srivastava, 2020; Djouzi and Beghdad-Bey, 2019; Ezugwu, 2020a). Xu and Tian (2015) explained the basic elements involved in the clustering process and broadly categorized existing clustering algorithms into two major perspectives: the traditional and modern ones. Xu and Wunsch (2005) reviewed major clustering algorithms for datasets appearing in Statistics, Computer Science, and Machine learning. Benabdellah et al. (2019) categorized clustering algorithms using the three V’s properties of Big Data: Volume, Variety, and Velocity. These three properties were used to explore the various categories of clustering algorithms. Adil et al. (2014) gave a concise survey of existing clustering algorithms and conducted extensive experiments to highlight the best-performing clustering algorithm for Big data analysis. Berkhin et al. (2001) reviewed clustering techniques in data mining, emphasizing object attribute type, large dataset scalability, handling high dimensional data, and finding irregularly shaped clusters.

许多关于聚类技术的综述存在于文献中（Xu和Wunsch，2005；Xu和Tian，2015；Benabdellah等人，2019；Adil等人，2014；Dafir等人，2021；Saxena等人，2017；Nagpal，2013；Oyelade等人，2016；Bindra和Mishra，2017；Singh和Srivastava，2020；Djouzi和Beghdad-Bey，2019；Ezugwu，2020a）。Xu和Tian（2015）解释了聚类过程中涉及的基本元素，并将现有的聚类算法广泛分类为两个主要视角：传统算法和现代算法。Xu和Wunsch（2005）回顾了在统计学、计算机科学和机器学习中出现的数据集的主要聚类算法。Benabdellah等人（2019）使用大数据的三个V特性——体量（Volume）、多样性（Variety）和速度（Velocity）对聚类算法进行了分类，这三个特性被用来探索聚类算法的各种类别。Adil等人（2014）对现有聚类算法进行了简明的调查，并进行了广泛的实验以突出表现最佳的大数据分析聚类算法。Berkhin等人（2001）回顾了数据挖掘中的聚类技术，强调了对象属性类型、大规模数据集的可扩展性、处理高维数据以及发现不规则形状的簇。

Dafir et al. (2021)’s work was on parallel clustering algorithms, classifying and summarizing them. He discussed the framework for each kind of parallel clustering algorithm. Saxena et al. (2017) presented a taxonomy of existing clustering algorithms, debating each algorithm’s various measures of similarity and evaluation criteria. Nagpal (2013) carried out a comparative analysis of the different clustering algorithms concerning both the mixed and categorical datasets with the observation that no clustering algorithm can be adjudged as best for handling a large dataset of either the mixed or categorical dataset. Oyelade et al. (2016) examined various clustering algorithms and their suitability for gene expression data to discover and provide helpful knowledge that will guarantee stability and a high degree of accuracy in the area. Jain (2010) summarized well-known clustering methods with a discussion on critical issues and challenges in the design of clustering algorithms. Jain et al. (1999) discussed emerging techniques for non-numeric constraints and large sets of patterns. Ezugwu et al. (2020a) presented an in-depth and systematic review of nature-inspired metaheuristic algorithms used for automatic clustering analysis focusing on the metaheuristic algorithms that have been employed to solve clustering problems over the last three decades.

Dafir等人（2021）的研究聚焦于并行聚类算法，对他们进行了分类和总结，并讨论了每种并行聚类算法的框架。Saxena等人（2017）提出了现有聚类算法的分类，辩论了每个算法的各种相似性度量和评估标准。Nagpal（2013）对不同聚类算法进行了比较分析，涉及混合数据集和分类数据集，观察到没有一种聚类算法可以被判定为最适合处理大规模的混合或分类数据集。Oyelade等人（2016）检查了各种聚类算法及其对基因表达数据的适用性，以发现并提供有助于保证该领域稳定性和高准确度的有用知识。Jain（2010）总结了知名的聚类方法，并讨论了设计聚类算法时的关键问题和挑战。Jain等人（1999）探讨了针对非数值约束和大型模式集的新技术。Ezugwu等人（2020a）深入系统地回顾了过去三十年中用于自动聚类分析的自然启发式元启发算法，重点关注那些已被用来解决聚类问题的元启发算法。

#### 4.2.1.6 Square error clustering 

The square error clustering method is a partitioning clustering method that assigns data points into a specified number of clusters based on the sum of square error criterion functions. The squared differences between each data point and the estimated center value for each stated group have been divided into the data point. In cases where the sum of squared error for a group of data objects is equal to zero, the cluster’s data points are identical (very close). The formula for Sum of Square Error is:

$$\text{𝑆𝑢𝑚 𝑜𝑓 𝑠𝑞𝑢𝑎𝑟𝑒 𝑒𝑟𝑟𝑜𝑟} = \Sigma_{𝑖=1}^n (𝑥𝑖 − 𝑥)^2$$

where 𝑛 represents the number of data points and 𝑥i represents the 𝑖th data point in the group and 𝑥 is the center object relative to the group. The k-means clustering algorithm is the best know squared error-based clustering algorithm (Xu and Wunsch, 2005).

##### i. K-Means Clustering

The K-Means clustering algorithm is a centroid-based partitioning technique in which data objects are distributed into a specified number of k clusters. The distribution is done through the use of an objective function which accesses the quality of the partition, ensuring that the similarities of objects within a cluster (intra-cluster similarity) is higher compared with objects in another cluster (inter-cluster similarity). KMeans clustering is a centroid-based technique, and it uses the mean to represent the centroid of a cluster. The centroid of a cluster is a measure of the center point of the cluster. Specified 𝑘 numbers of data points/objects are randomly selected from a set of the existing data points as the representative center for 𝑘 clusters. The Euclidean distance between the remaining data points and each assumed center point is then iteratively measured. The obtained value assigns the data point to the cluster with the smallest distance. The intracluster similarity is improved each time a new data point is given to the cluster by computing a new mean using the objects previously assigned to the clusters. The new mean is then used to reassign the data objects. This procedure is repeated severally until stability is achieved.

K-Means聚类算法是一种基于质心的划分技术，其中数据对象被分配到指定数量的k个簇中。分配过程是通过使用一个目标函数来完成的，该函数评估分区的质量，确保同一簇内对象之间的相似度（簇内相似性）高于其他簇中的对象（簇间相似性）。KMeans聚类是一种基于质心的技术，它使用平均值来表示簇的质心。簇的质心是该簇中心点的一种度量。从现有的数据点集中随机选择指定数量𝑘的数据点/对象作为𝑘个簇的代表性中心。然后迭代地测量剩余数据点与每个假定中心点之间的欧几里得距离。获得的值将数据点分配给距离最小的簇。每当有新的数据点被分配给簇时，通过计算之前分配给簇的对象的新均值来提升簇内相似度。新均值随后用于重新分配数据对象。这个过程会重复多次，直到达到稳定状态。

The sum of square function for the Euclidean distances produces compact and well-separated clusters. The K-Means algorithm tries to minimize the sum of the squared error criterion (Ezugwu, 2020a; Hartigan and Wong, 1979; MacQueen, 1967). The major problems identified with K-Means clustering algorithms include the problem of the initial definition of the number of clusters at the algorithm’s onset. An efficient and universal method for determining the initial number of clusters and partition is not found. K-Means algorithm is reported to be very sensitive to initial centroid selection such that suboptimal solution may be produced when wrongfully chosen (Punit, 2018). Also, convergence to global optimum cannot be guaranteed. Using means as a centroid limits the K-Means algorithm’s application to data objects with numerical variables (Xu and Wunsch, 2005). Not only these, but the K-Means algorithm is also sensitive to outliers (objects that are quite far from the cluster centroid are forced into the cluster, distorting the cluster’s shape (Saxena et al., 2017). It works on the assumption that the variance of the distribution of each attribute is spherical and thus produces a roughly equal number of observations. Moreover, the memory space requirement is high and the number of iterations to obtain a stable distribution is unknown. Due to the simplicity of implementation and low computation complexity (Jain, 2010), the K-Means algorithm is still popular and widely used today (Ezugwu, 2020a).

平方和函数在欧氏距离下能生成紧凑且分离良好的簇。K 均值算法试图最小化平方误差准则（Ezugwu, 2020a; Hartigan and Wong, 1979; MacQueen, 1967）。该算法存在的主要问题包括：需在算法启动时预先定义簇的数量。目前尚未发现一种高效且通用的方法来确定初始簇数和划分方式。已有研究表明，K 均值算法对初始质心的选择极为敏感，错误的质心选择可能导致次优解（Punit, 2018）。此外，该算法无法保证收敛到全局最优解。使用均值作为质心的方式，将 K 均值算法的应用限制在具有数值型变量的数据对象上（Xu and Wunsch, 2005）。不仅如此，K 均值算法对离群点也非常敏感 —— 那些距离簇心较远的对象会被强行归入该簇，从而扭曲簇的形状（Saxena et al., 2017）。该算法基于各属性分布方差呈球形的假设，因此生成的观测数量大致相等。此外，该算法还存在内存空间需求高、获得稳定分布所需迭代次数未知等问题。尽管如此，由于其实现简单且计算复杂度低（Jain, 2010），K 均值算法至今仍被广泛使用（Ezugwu, 2020a）。

Some research work extending K-Means has been reported. For
example, the G-means (Hamerly and Elkan, 2004) and the X-means
algorithms (Pelleg, 2000). The sum of square function for the Euclidean
distances for the K-Means algorithm is given as:

$$𝑑_{𝑖𝑘} =\Sigma^𝑚_{𝑗=1}(𝑥_{𝑖𝑗} − 𝑐{𝑘𝑗})^2$$

The K-Means is arguably the most popular clustering method but is plagued with drawbacks such as poor scalability, sensitivity to initialization and outliers, assumed knowledge of cluster count, and local production rather than the global optimum. It is noteworthy to mention that the most recent extensions and improvements on the K-Means seek to advance the state-of-the-art in addressing these issues.

K 均值算法无疑是最受欢迎的聚类方法，但存在诸多缺点，例如可扩展性差、对初始化和离群点敏感、需要预先知道簇数，以及容易陷入局部最优而非全局最优解。值得注意的是，近期针对 K 均值算法的扩展和改进研究正致力于在解决这些问题上推动技术前沿的发展。

##### ii. K-MCI (K-Means modified cohort intelligence) Clustering algorithm

The K-MCI (K-Means modified cohort intelligence) is an efficient hybrid evolutionary data clustering algorithm that combines the KMeans algorithm with modified cohort intelligence (Krishnasamy et al., 2014). Cohort Intelligence(CI) is an optimization algorithm inspired by the natural and societal tendency of cohort candidates/individuals learning from one another. It was proposed by Kulkarni et al. (2013). In cohort intelligence, while observing every other candidate, each candidate tries to improve their behavior. The MCI is a modified cohort intelligence with improved accuracy and speed of convergence of the traditional CI. In K-MCI, the K-Means algorithm enhances the candidate’s behavior generated by MCI, annexing the advantages of the K-Means algorithm and that of the MCI. K-MCI converges more quickly with greater clustering accuracy without being trapped in the local optimum.

K-MCI（K 均值改进队列智能）是一种高效的混合进化数据聚类算法，它将 K 均值算法与改进的队列智能（MCI）相结合（Krishnasamy 等，2014）。队列智能（CI）是由 Kulkarni 等（2013）提出的一种受群体候选者 / 个体相互学习的自然与社会行为启发的优化算法。在队列智能中，每个候选者通过观察其他候选者来尝试改进自身行为。MCI 作为改进的队列智能算法，在传统 CI 基础上提升了收敛速度和准确性。K-MCI 通过 K 均值算法增强了 MCI 生成的候选行为，融合了两种算法的优势。该算法不仅收敛速度更快、聚类精度更高，还能有效避免陷入局部最优。

##### iii. ELM K-Means (Extreme Learning Machine K-Means)

In ELM K-Means, the extreme learning machine (ELM) method is incorporated into the K-Means clustering algorithm (Alshamiri et al., 2015). The ELM method functions in projecting the dataset into a high dimensional feature space, and the K-Means algorithm is used to cluster the dataset using the Euclidean distance in the feature space to measure the similarity between the objects. The ELM proposed by Huang et al. (2006) is a new learning algorithm that randomly generates hidden nodes for single hidden layer feedforward neural networks(SLFNs) and determines the output weight of the SLFNs analytically. ELM is credited with a meager computational cost for its operations and has been used in finding the solution to classification and regression problems.

在 ELM K-Means 算法中，极限学习机（ELM）方法被融入 K 均值聚类算法（Alshamiri 等，2015）。ELM 方法的作用是将数据集投影到高维特征空间，随后 K 均值算法利用该特征空间中的欧氏距离对数据对象进行相似度度量并实现聚类。由 Huang 等（2006）提出的 ELM 是一种新型学习算法，其通过随机生成单隐层前馈神经网络（SLFNs）的隐层节点，并以解析方式确定网络的输出权重。ELM 因其运算的计算成本极低而广受认可，已被用于解决分类和回归问题。

##### iv. K-means based multiview clustering methods and K-means subspace clustering models

The generation of high dimensional data due to the social network’s rapid development has been a significant challenge to the traditional K-means clustering generally tagged as the curse of dimensionality. Redundant features and noises in such data make efficient clustering of such data very difficult. The K-means based multiview clustering methods are developed to provide simple and efficient algorithms for accurately exploring shared information in multiview data. Zheng et al. (2018) proposed a robust discriminative multiview K-means clustering with feature selection and group sparsity learning. The proposed algorithm addressed the problem of extreme time consuming and sensitivity to outliers that is common with clustering of high-dimensional feature space. It efficiently handles the curse of dimensionality by using group sparsity constraints for selecting the most important views and the most relevant features.

由于社交网络的快速发展而产生的高维数据对传统的K均值聚类构成了重大挑战，这通常被称为“维度灾难”。此类数据中的冗余特征和噪声使得对其进行高效聚类变得非常困难。基于K均值的多视图聚类方法被开发出来，旨在为精确探索多视图数据中的共享信息提供简单而高效的算法。Zheng等人（2018）提出了一种具有特征选择和组稀疏性学习的鲁棒判别多视图K均值聚类算法。该算法解决了在高维特征空间聚类中常见的极端耗时和对异常值敏感的问题。通过使用组稀疏性约束来选择最重要的视图和最相关的特征，它能够有效应对维度灾难。

In handling high dimensional data of a real-world application, using eigenvalue decomposition by existing K-means subspace clustering algorithm to find an approximate solution is less efficient. Moreover, their loss functions exhibit sensitivity to outliers or suffer small loss errors (Wang et al., 2019). A new adaptive Multiview subspace clustering method was recently proposed by Yan et al. (2020) for integrating heterogeneous data in low-dimensional feature space. Their work extended K-Means clustering with feature learning capability for handling high-dimensional data. Wang et al. (2019) developed a fast adaptive K-means (FAKM) type subspace clustering model embedded with a mechanism for flexible cluster indicator using an adaptive loss function.

在处理实际应用中的高维数据时，使用现有的K均值子空间聚类算法通过特征值分解找到近似解的效率较低。此外，它们的损失函数对外异常值敏感或存在较小的损失误差（Wang等人，2019）。最近，Yan等人（2020）提出了一种新的自适应多视图子空间聚类方法，用于在低维特征空间中整合异构数据。他们的工作扩展了K均值聚类，增加了特征学习能力，以处理高维数据。Wang等人（2019）开发了一种快速自适应K均值（FAKM）类型的子空间聚类模型，该模型嵌入了一个使用自适应损失函数的灵活聚类指示机制。

According to Wang et al. the existing methods of combining subspace learning with K-means clustering still exhibit some limitations. These include: no thorough capturing of discriminative information in low-dimensional subspace, consideration of intrinsic geometric information is rare, and the vulnerability to noises of the optimizing procedure of a discrete cluster indicator. They proposed a robust dimension reduction for clustering with a local adaptive learning algorithm to address these limitations. The proposed algorithm adaptively explores the discriminative information by unifying K-means clustering with local adaptive subspace learning.

据Wang等人所述，现有的将子空间学习与K均值聚类相结合的方法仍然存在一些局限性。这些局限包括：未能在低维子空间中彻底捕获判别信息，很少考虑内在几何信息，以及离散聚类指示优化过程易受噪声影响。为了解决这些问题，他们提出了一种结合局部自适应学习算法的鲁棒降维聚类方法。所提出的算法通过将K均值聚类与局部自适应子空间学习统一起来，自适应地探索判别信息。

