# K-means clustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data

## Abstract

Advances in recent techniques for scientific data collection in the era of big data allow for the systematic accumulation of large quantities of data at various data-capturing sites. Similarly, exponential growth in the development of different data analysis approaches has been reported in the literature, amongst which the K-means algorithm remains the most popular and straightforward clustering algorithm. The broad applicability of the algorithm in many clustering application areas can be attributed to its implementation simplicity and low computational complexity. However, the K-means algorithm has many challenges that negatively affect its clustering performance. In the algorithm’s initialization process, users must specify the number of clusters in a given dataset apriori while the initial cluster centers are randomly selected. Furthermore, the algorithm’s performance is susceptible to the selection of this initial cluster and for large datasets, determining the optimal number of clusters to start with becomes complex and is a very challenging task. Moreover, the random selection of the initial cluster centers sometimes results in minimal local convergence due to its greedy nature. A further limitation is that certain data object features are used in determining their similarity by using the Euclidean distance metric as a similarity measure, but this limits the algorithm’s robustness in detecting other cluster shapes and poses a great challenge in detecting overlapping clusters. Many research efforts have been conducted and reported in literature with regard to improving the K-means algorithm’s performance and robustness. The current work presents an overview and taxonomy of the K-means clustering algorithm and its variants. The history of the K-means, current trends, open issues and challenges, and recommended future research perspectives are also discussed.

在大数据时代，科学数据收集技术的进步使得在不同的数据采集点系统地积累大量数据成为可能。同样，在文献中报道了不同数据分析方法的指数级增长，其中K均值算法仍然是最流行且最直接的聚类算法。该算法在许多聚类应用领域的广泛应用可以归因于其实施的简单性和较低的计算复杂度。然而，K均值算法存在许多负面影响其聚类性能的挑战。在其初始化过程中，用户必须事先指定给定数据集中的聚类数量，同时初始聚类中心是随机选择的。此外，该算法的性能对初始聚类的选择非常敏感，对于大型数据集而言，确定开始时的最佳聚类数量变得复杂且是一项非常具有挑战性的任务。而且，由于其贪婪性质，初始聚类中心的随机选择有时会导致最小局部收敛。

进一步的限制在于，使用欧几里得距离度量作为相似性度量来确定数据对象之间的相似性，但这限制了算法检测其他簇形状的鲁棒性，并对检测重叠簇构成巨大挑战。关于改进K均值算法的性能和鲁棒性，已经进行了许多研究并在文献中有所报告。当前的工作概述并分类了K均值聚类算法及其变体。讨论了K均值的历史、当前趋势、开放问题和挑战，以及推荐的未来研究视角。

## 1. Introduction

Extracting meaningful and tangible information from collected data is the primary goal of data mining [4]. However, most data are collected in arbitrary forms and categories, making such data difficult to analyse, especially when the data objects’ features are unknown. Appropriate organization of unlabeled data is an aspect of data mining handled by cluster analysis. The meaningful grouping of such unlabeled data is regarded as data clustering. The goal is to group unlabeled data so that the data objects whose characteristics and attributes are similar are together in a cluster such that the similarities of data objects within the same clusters are higher when compared with other clusters’ data objects. In other words, data clustering analysis classifies unlabeled data to ensure higher intra-cluster similarity and lower inter-cluster similarity [59]. The process of clustering analysis can be likened to the learning process, which involves specific predictive behavior associated with unsupervised learning when handling unlabeled datasets [55]. Fig. 1 clearly illustrates this spectrum of different categories of learning problems of interest in pattern recognition and machine learning, as discussed in Jain [95].

从收集的数据中提取有意义且具体的资讯是数据挖掘的主要目标[4]。然而，大多数数据是以任意形式和类别收集的，这使得这些数据难以分析，特别是当数据对象的特征未知时。未标记数据的适当组织是数据挖掘中由聚类分析处理的一个方面。对这种未标记数据进行有意义的分组被称为数据聚类。其目标是将未标记的数据分组，使具有相似特性和属性的数据对象聚集在同一个簇中，从而使同一簇内数据对象之间的相似度高于其他簇中的数据对象。换句话说，数据聚类分析对未标记数据进行分类，以确保簇内相似度更高而簇间相似度更低[59]。聚类分析的过程可以比作学习过程，这涉及到处理未标记数据集时与无监督学习相关的特定预测行为[55]。图1清晰地展示了模式识别和机器学习中不同类别的学习问题的范围，正如Jain [95]中讨论的那样。

Cluster analysis has been successfully applied to address data clustering problems in different domains such as medical science, manufacturing, robotics, the financial sector, privacy protection, artificial intelligence, urban development, aviation, industries, sales, and marketing [61,7,180,59,20,111,49]. Extracting useful information from data in these domains is essential for providing better services and generating more profits [181,148,172]. Real-world data generated are mostly voluminous, unlabeled, and of different dimensions. This makes data clustering difficult. Pre-identifying the number of clusters in a real-world dataset cannot be quickly done. Therefore, determining the optimal number of clusters in a real-world dataset characterized by high density and dimensionality is quite tricky for standard clustering algorithms. This poses a significant challenge to conventional clustering algorithms in which the number of clusters must be specified as input to the algorithm.

聚类分析已成功应用于解决医学、制造业、机器人学、金融、隐私保护、人工智能、城市发展、航空、工业、销售和市场营销等不同领域的数据聚类问题 [61,7,180,59,20,111,49]。从这些领域的数据中提取有用信息对于提供更优质的服务和创造更多利润至关重要 [181,148,172]。现实世界生成的数据大多具有海量、未标注和多维度特征，这使得数据聚类变得困难。预先确定现实数据集中的簇数并非易事，因此，对于具有高密度和高维度特征的现实数据集，确定其最优簇数对标准聚类算法而言极具挑战性。这对需要预先指定簇数作为输入的传统聚类算法构成了重大挑战。

Algorithms for data clustering are grouped into two major categories [97,224,68,60], namely, hierarchical clustering algorithms and partitional clustering algorithms. Hierarchical clustering algorithms partition data objects into clusters in a hierarchical form either in a bottom-up approach (agglomerative method) or a top-down approach (divisive method). In the agglomerative method, individual data objects are merged iteratively based on their similarity. In the divisive method, the initial dataset is taken as a single cluster and broken down iteratively using data object similarity until each data object forms a single cluster or a set criterion is met. The hierarchical clustering algorithm produces a dendrogram of merged (agglomerative) or split (divisive) data objects depicting the corresponding cluster hierarchy generated as output for the cluster analysis [60]. The dendrogram is a pictorial representation of the data objects’ nested grouping showing the similarity level at which each grouping changes [97].

数据聚类算法主要分为两大类 [97,224,68,60]，即层次聚类算法和划分聚类算法。层次聚类算法通过自底向上（凝聚法）或自顶向下（分裂法）的层次方式对数据对象进行划分。在凝聚法中，各个数据对象基于相似性逐步合并；在分裂法中，初始数据集被视为单个簇，通过数据对象相似性逐步分解，直至每个数据对象形成单例簇或满足特定准则。层次聚类算法生成合并（凝聚法）或分裂（分裂法）数据对象的树状图，该图直观展示了聚类分析输出的簇层次结构 [60]。树状图以嵌套分组形式呈现数据对象的层次关系，显示各分组变化时的相似性水平 [97]。

In the partitional clustering approach, a single partition of the initial dataset is produced instead of a clustering structure of a dendrogram. Clusters are produced in a heuristic approach while optimizing a criterion function defined globally on all the data objects in the set or locally on the subset of the data objects [246,9,189]. Optimizing a criterion function on a set of the data objects using a combinatorial search of all possible values to get the optimum value is computationally prohibitive. Therefore, partitional clustering algorithms require the specification of different k values supplied at different runs to obtain the best configuration to produce the optimum clusters.

在划分聚类方法中，算法生成初始数据集的单一划分，而非树状图结构的聚类层次。聚类通过启发式方法生成，同时优化定义在全局所有数据对象或局部数据对象子集上的准则函数 [246,9,189]。通过组合搜索所有可能值来优化数据对象集合的准则函数以获取最优解的计算成本极高。因此，划分聚类算法需要在不同运行中指定不同的 k 值，以获得生成最优簇的最佳配置。

K-means clustering algorithm was proposed independently by different researchers, including Steinhaus [203], Lloyd [132], MacQueen [135], and Jancey [98] from different disciplines during the 1950s and 1960s [171]. These researchers’ various versions of the algorithms show four common processing steps with differences in each step [171]. The K-means clustering algorithm generates clusters using the cluster’s object mean value [197,34]. In the standard K-means algorithm, the cluster number is required as a user parameter and is used in the arbitrary cluster center selection from the dataset. However, the K-means algorithm may converge to a local minimum because of its greedy nature [95]. Therefore, it requires several runs for a given k value with different initial cluster center selections to obtain the optimal cluster result [243,59,19]. In addition, the standard algorithm detects ball-shaped or spherical clusters only because of the use of the Euclidean metric as its distance measure [95]. A typical K-means clustering process is illustrated in Fig. 2.With a set of input data supplied to the K-means clustering algorithm, the centroid vector c can easily be identified with K being the number of centroids defined by the user. Fig. 2a illustrates a data set in 2D space distributed randomly with 100 < xi; yi < 100, and Fig. 2b presents the K-means clustering result with the number of centroids set to K ¼ 3.

K 均值聚类算法由不同学科的研究者在 20 世纪 50 至 60 年代独立提出，包括 Steinhaus [203]、Lloyd [132]、MacQueen [135] 和 Jancey [98][171]。这些研究者提出的不同版本算法均包含四个通用处理步骤，但每个步骤存在差异 [171]。K 均值聚类算法通过计算簇的对象均值生成聚类 [197,34]。在标准 K 均值算法中，用户需指定簇数 k 作为参数，并从数据集中随机选择初始簇中心。然而，由于其贪心性质，该算法可能收敛于局部最小值 [95]。因此，对于给定 k 值，需通过多次运行并选择不同的初始簇中心以获取最优聚类结果 [243,59,19]。此外，由于采用欧氏距离作为测度，标准算法仅能检测球形或类球形簇 [95]。典型 K 均值聚类过程如图 2 所示：当向算法输入数据集后，可轻松确定质心向量 c（其中 K 为用户定义的质心数量）。图 2a 展示了在二维空间中随机分布的数据集（100 < xi; yi < 100），图 2b 则呈现了当质心数设置为 K=3 时的 K 均值聚类结果。

Despite these limitations, the K-means clustering algorithm is credited with flexibility, efficiency, and ease of implementation. It is also among the top ten clustering algorithms in data mining [59,217,105,94]. The simplicity and low computational complexity have given the K-means clustering algorithm a wide acceptance in many domains for solving clustering problems. Several K-means clustering algorithm variants have been developed to enhance its performance. This work presents an overview of the K-means clustering algorithm and its variants with a proposed taxonomy for the variants. The algorithm’s research progression from its inception, the current trends, open issues, and challenges with recommended future research perspectives are also discussed in detail.

尽管存在上述局限性，K 均值聚类算法仍因其灵活性、高效性和易实现性而广受赞誉。该算法还跻身数据挖掘领域十大聚类算法之列 [59,217,105,94]。其简单性和低计算复杂度使 K 均值聚类算法在众多领域获得广泛认可，用于解决聚类问题。目前已开发出多种 K 均值聚类算法变体以提升其性能。本文将对 K 均值聚类算法及其变体进行综述，并提出针对这些变体的分类体系。同时，还将详细探讨该算法自诞生以来的研究演进历程、当前研究趋势、开放性问题与挑战，以及建议的未来研究方向。

## 7. Conclusion

The K-means clustering algorithm is known for its simplicity and is applied in clustering datasets from different domains. Despite this advantage, its performance is greatly hampered due to some of the problems inherent in its implementation. As a result, much research has been conducted to improve the algorithm’s general performance. This review work has been able to identify the various limitations of the standard algorithm and the numerous variants developed to solve the identified problems up to the time of this review work. This paper will benefit researchers working on extending the existing variants to achieve a more robust and scalable K-means-based clustering technique, as well as practitioners interested in using the state-of-the-art variants of the standard algorithm to meet the data clustering needs in their domain. Practitioners having a problem with the existing K-means-based algorithm can easily identify which variant will adequately serve their application need, or identify the method that can be adopted to improve their existing algorithm.

K 均值聚类算法以其简洁性著称，被广泛应用于不同领域的数据集聚类。尽管具备这一优势，但其性能仍因实现过程中固有的某些问题而受到严重影响。为此，学术界开展了大量研究以提升该算法的综合性能。本综述研究不仅梳理了标准算法的各项局限性，还系统总结了截至综述完成时为解决这些问题而开发的众多改进变体。本文将为致力于扩展现有变体以构建更鲁棒、可扩展的基于 K 均值聚类技术的研究人员提供参考，同时也为希望采用标准算法先进变体满足其领域数据聚类需求的从业者提供指导。对于在现有基于 K 均值算法应用中遇到问题的从业者，本文可帮助其快速定位能有效满足应用需求的算法变体，或确定可用于改进现有算法的方法。

The findings of this study reveal that much focus has been placed on solving the initialization problems of K-means algorithms with little focus on addressing the problem of mixed data type. New technology such as MapReduce, parallel implementation, and Kernel-based implementation is being researched to address big data clustering using the standard algorithm. The hybridization of the standard algorithm with a metaheuristic algorithm for automatic clustering is a new and upcoming area with little work done so far. Only a few existing metaheuristic algorithms have been reportedly combined with the standard algorithm to solve the problem of convergence into local optimal. Future research can investigate automatic clustering algorithms that hybridize the standard or variants with other swarm intelligence metaheuristic algorithms. Researchers and practitioners seeking to design improved automatic clustering based on the standard algorithm or its variants will find this survey very useful.

本研究发现表明，现有研究主要聚焦于解决 K 均值算法的初始化问题，而对混合数据类型问题的关注较少。目前，MapReduce 框架、并行实现以及基于核的实现等新技术正被用于研究如何通过标准算法解决大数据聚类问题。将标准算法与元启发式算法相融合以实现自动聚类是一个新兴领域，目前相关研究较少。据报道，仅有少数现有元启发式算法与标准算法结合用于解决收敛至局部最优的问题。未来研究可探索将标准算法或其改进变体与其他群体智能元启发式算法相融合的自动聚类算法。本综述将为致力于基于标准算法或其变体设计改进型自动聚类方法的研究人员和从业者提供重要参考。